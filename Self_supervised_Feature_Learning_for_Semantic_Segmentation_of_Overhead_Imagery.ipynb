{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Supervised Feature Learning for Semantic Segmentation of Overhead Imagery\n",
    "#### Suriya Singh\\*, Anil Batra\\*, Guan Pang, Lorenzo Torresani, Saikat Basu, Manohar Paluri, and C. V. Jawahar\n",
    "#### In BMVC, 2018\n",
    "[LINK](http://suriyasingh.github.io/Suriya_BMVC18_Self_supervised_segmentation_overhead.pdf)\n",
    "![Overview](https://docs.google.com/drawings/d/e/2PACX-1vSTOkisVLcJGwkD6NyNqbBUG8x4yG8NXCDuNYXB7LQh7S4zPFAUNJtU5cwlzv5uNXJk7ijd63xT34WP/pub?w=3075&h=912)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Inpainting (Pathak *et al.*, CVPR 2016) [link](http://people.eecs.berkeley.edu/~pathak/context_encoder/)\n",
    "* Learn representations by performing semantic inpainting using input $\\hat{x}$ and outputs $\\bar{x}$ as a reconstruction of $x$.\n",
    "* Input:\n",
    "    * Image is an RGB image\n",
    "    * Mask is a binary image where 0 corresponds to regions to be erased and 1 corresponds to regions where image values are kept intact\n",
    "    * Image: $x = B \\times 3 \\times H \\times W$\n",
    "    * Mask: $M = B \\times 3 \\times H \\times W$\n",
    "    * $\\hat{x} = M \\odot x$\n",
    "* Output:\n",
    "    * $\\bar{x} = B \\times 3 \\times H \\times W$\n",
    "* Loss:\n",
    "    * $L_{rec} = \\frac{1}{\\sum (1 - M)}||(1−M)\\odot(x−F(M \\odot x))||^{2}_{2}$\n",
    "    * $L_{con} = \\frac{1}{\\sum M}||M\\odot(x−F((1-M) \\odot x))||^{2}_{2}$\n",
    "    * $Loss = w_{rec}L_{rec} + w_{con}L_{con}$\n",
    "\n",
    "### Adversarial Mask Prediction\n",
    "* Predicts $M$ using coach network\n",
    "* Input:\n",
    "    * RGB image\n",
    "* Output:\n",
    "    * $M = B \\times 3 \\times H \\times W$\n",
    "* Loss:\n",
    "    * $L_{coach} = 1 - L_{rec}$\n",
    "\n",
    "![Context inpainting with adversarial mask prediction](https://docs.google.com/drawings/d/e/2PACX-1vQhNKVtrNOBLDdw6cZfgAoO7SCQUJmiQNaMaeb5c3_oj_cYtUpwoGbvP1Uyu-Ai66h0x5jfbzKBexkW/pub?h=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils.dataloaders import context_inpainting_dataloader, segmentation_data_loader\n",
    "from models import resnet18_encoderdecoder, resnet18_encoderdecoder_wbottleneck\n",
    "from models import resnet18_coach_vae\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## fix seeds\n",
    "torch.cuda.manual_seed(7)\n",
    "torch.manual_seed(7)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '/home/suriya/projects/datasets/'\n",
    "model_root = '/home/suriya/projects/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'potsdam'                                    #options are: spacenet, potsdam, deepglobe_roads, deepglobe_lands\n",
    "architecture = 'resnet18_autoencoder_no_bottleneck'    #options are: resnet18_autoencoder, resnet18_encoderdecoder_wbottleneck\n",
    "use_coach = True                                       #options are: True or Flase\n",
    "self_supervised_split = 'train_crops'                  #options are: train_10crops, train_25crops, train_50crops, train_crops\n",
    "supervised_split = 'train_10crops'                     #options are: train_10crops, train_25crops, train_50crops, train_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset + '_' + architecture                #model file suffix\n",
    "\n",
    "if use_coach:\n",
    "    experiment += '_' + 'use_coach'\n",
    "\n",
    "mean_bgr = np.array([85.5517787014, 92.6691667083, 86.8147645556])       # mean BGR values of images\n",
    "std_bgr = np.array([32.8860206505, 31.7342205253, 31.5361127226])        # standard deviation BGR values of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set data paths\n",
    "splits = None\n",
    "train_image_list_path = None\n",
    "train_img_root = None\n",
    "train_gt_root = None\n",
    "\n",
    "val_image_list = None\n",
    "val_img_root = None\n",
    "val_gt_root = None\n",
    "\n",
    "nClasses = None\n",
    "ignore_class = None\n",
    "\n",
    "\n",
    "if dataset == 'spacenet':\n",
    "    train_img_root = dataset_root + 'spacenet/spacenet_processed/train/images/'\n",
    "    train_gt_root = dataset_root + 'spacenet_processed/train/gt/'\n",
    "\n",
    "    val_img_root = dataset_root + 'spacenet/spacenet_processed/val/images/'\n",
    "    val_gt_root = dataset_root + 'spacenet/spacenet_processed/val/gt/'\n",
    "    val_image_list = dataset_root + 'spacenet/splits/val_crops.txt'\n",
    "\n",
    "    train_image_list_path = dataset_root + 'spacenet/splits/'\n",
    "\n",
    "    nClasses = 2                ### number of classes for pixelwise classification\n",
    "    out = 'seg'                 ### process ground-truth as binary segmentation\n",
    "\n",
    "elif dataset == 'potsdam':\n",
    "    train_img_root = dataset_root + 'potsdam/processed/train/images/'\n",
    "    train_gt_root = dataset_root + 'potsdam/processed/train/gt/'\n",
    "\n",
    "    val_img_root = dataset_root + 'potsdam/processed/val/images/'\n",
    "    val_gt_root = dataset_root + 'potsdam/processed/val/gt/'\n",
    "    val_image_list = dataset_root + 'potsdam/splits/val_crops.txt'\n",
    "\n",
    "    train_image_list_path = dataset_root + 'potsdam/splits/'\n",
    "    nClasses = 6                ### number of classes for pixelwise classification\n",
    "    out = None                  ### do not process ground-truth\n",
    "\n",
    "elif dataset == 'deepglobe_roads':\n",
    "    train_img_root = dataset_root + 'deepglobe_roads/processed/train/images/'\n",
    "    train_gt_root = dataset_root + 'deepglobe_roads/processed/train/gt/'\n",
    "\n",
    "    val_img_root = dataset_root + 'deepglobe_roads/processed/val/images/'\n",
    "    val_gt_root = dataset_root + 'deepglobe_roads/processed/val/gt/'\n",
    "    val_image_list = dataset_root + 'deepglobe_roads/splits/val_crops.txt'\n",
    "\n",
    "    train_image_list_path = dataset_root + 'deepglobe_roads/splits/'\n",
    "\n",
    "    nClasses = 2                ### number of classes for pixelwise classification\n",
    "    out = 'seg'                 ### process ground-truth as binary segmentation\n",
    "\n",
    "elif dataset == 'deepglobe_lands':\n",
    "    train_img_root = dataset_root + 'deepglobe_lands/processed/train/images/'\n",
    "    train_gt_root = dataset_root + 'deepglobe_lands/processed/train/gt/'\n",
    "\n",
    "    val_img_root = dataset_root + 'deepglobe_lands/processed/val/images/'\n",
    "    val_gt_root = dataset_root + 'deepglobe_lands/processed/val/gt/'\n",
    "    val_image_list = dataset_root + 'deepglobe_lands/splits/val_crops.txt'\n",
    "\n",
    "    train_image_list_path = dataset_root + 'deepglobe_lands/splits/'\n",
    "    nClasses = 7                ### number of classes for pixelwise classification\n",
    "    out = None                  ### do not process ground-truth\n",
    "    ignore_class = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erase_shape = [16, 16]         ### size of each block used to erase image\n",
    "erase_count = 16               ### number of blocks to erase from image\n",
    "rec_weight = 0.99            ### loss = rec_weight*loss_rec+ (1-rec_weight)*loss_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    context_inpainting_dataloader(img_root = train_img_root, image_list = train_image_list_path+self_supervised_split+'.txt', suffix=dataset,\n",
    "                                  mirror = True, resize=True, resize_shape=[256, 256], rotate = True, \n",
    "                                  erase_shape = erase_shape, erase_count = erase_count), \n",
    "    batch_size=128, num_workers=10, shuffle = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    context_inpainting_dataloader(img_root = val_img_root, image_list = val_image_list, suffix=dataset,\n",
    "                                  mirror = False, resize=False, resize_shape=[256, 256], rotate = False, \n",
    "                                  crop = True, erase_shape = erase_shape, erase_count = erase_count), \n",
    "    batch_size=32, num_workers=10, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_np(input_, mask, target, output = None):\n",
    "    input_ = np.asarray(input_.numpy().transpose(1, 2, 0)+ mean_bgr[np.newaxis, np.newaxis, :], dtype=np.uint8)[:,:,::-1]\n",
    "    mask = np.asarray(mask[0].numpy(), dtype=np.uint8)\n",
    "    target = np.asarray(3*std_bgr*(target.numpy().transpose(1, 2, 0)) + mean_bgr[np.newaxis, np.newaxis, :], dtype=np.uint8)[:,:,::-1]\n",
    "    \n",
    "    if output is not None:\n",
    "        output = np.asarray(3*std_bgr*(output.numpy().transpose(1, 2, 0)) + mean_bgr[np.newaxis, np.newaxis, :], dtype=np.uint8)[:,:,::-1]\n",
    "    return input_, mask, target, output\n",
    "\n",
    "def visualize_self_sup(cols = 3, net = None, coach = None, use_coach_masks = False):\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=cols, figsize=(9, 9))\n",
    "    \n",
    "    for batch_idx, (inputs_, masks, targets) in enumerate(val_loader):\n",
    "        if coach is None:\n",
    "            inputs_ = inputs_*masks.float()\n",
    "        else:\n",
    "            masks, _, _ = coach.forward(inputs_.cuda(), alpha = 100, use_coach = use_coach_masks)\n",
    "            inputs_ = inputs_*masks.float().cpu()\n",
    "        \n",
    "        outputs = None\n",
    "        if cols == 4:\n",
    "            outputs = net.forward(inputs_.cuda()).cpu().data\n",
    "            input_, mask, target, output = torch_to_np(inputs_[0].cpu(), masks[0].cpu(), targets[0].cpu(), outputs[0].cpu())\n",
    "        else:\n",
    "            input_, mask, target, _ = torch_to_np(inputs_[0].cpu(), masks[0].cpu(), targets[0].cpu())\n",
    "        axs[batch_idx, 0].imshow(input_)\n",
    "        axs[batch_idx, 1].imshow(mask, cmap='gray')\n",
    "        axs[batch_idx, 2].imshow(target)\n",
    "        if cols == 4:\n",
    "            axs[batch_idx, 3].imshow(output)\n",
    "        if batch_idx == 3:\n",
    "            break\n",
    "     \n",
    "    axs[0,0].set_title('input', fontsize=18)\n",
    "    axs[0,1].set_title('mask', fontsize=18)\n",
    "    axs[0,2].set_title('target', fontsize=18)\n",
    "    if cols == 4:\n",
    "        axs[0,3].set_title('semantic inpainting', fontsize=18)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_self_sup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18_encoderdecoder().cuda()\n",
    "net_coach = None\n",
    "\n",
    "if use_coach:\n",
    "    net_coach = resnet18_coach_vae(drop_ratio=0.75).cuda()\n",
    "    \n",
    "net_optimizer = None\n",
    "coach_optimizer = None\n",
    "best_loss = 1e5\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "coach_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_context_inpainting(epoch, net, net_optimizer, coach = None, use_coach_masks = False):\n",
    "    progbar = tqdm_notebook(total=len(train_loader), desc='Train')\n",
    "    net.train()\n",
    "    \n",
    "    if coach is not None:\n",
    "        coach.eval()\n",
    "        \n",
    "    train_loss.append(0)\n",
    "    for batch_idx, (inputs_, masks, targets) in enumerate(train_loader):\n",
    "        net_optimizer.zero_grad()\n",
    "        inputs_, masks, targets = Variable(inputs_.cuda()), Variable(masks.cuda().float()), Variable(targets.cuda())\n",
    "        \n",
    "        if coach is not None:\n",
    "            masks, _, _ = coach.forward(inputs_, alpha = 100, use_coach = use_coach_masks)\n",
    "            \n",
    "        outputs_1 = net(inputs_*masks)\n",
    "        mse_loss = (outputs_1 - targets)**2\n",
    "        mse_loss = -1*F.threshold(-1*mse_loss, -2, -2)\n",
    "        loss_rec = torch.sum(mse_loss*(1-masks))/torch.sum(1-masks)\n",
    "        \n",
    "        outputs_2 = net(inputs_*(1-masks))\n",
    "        mse_loss = (outputs_2 - targets)**2\n",
    "        mse_loss = -1*F.threshold(-1*mse_loss, -2, -2)\n",
    "        loss_con = torch.sum(mse_loss*masks)/torch.sum(masks)\n",
    "        \n",
    "        total_loss = rec_weight*loss_rec + (1-rec_weight)*loss_con\n",
    "        total_loss.backward()\n",
    "        \n",
    "        net_optimizer.step()\n",
    "        \n",
    "        train_loss[-1] += total_loss.data\n",
    "        progbar.set_description('Train (loss=%.4f)' % (train_loss[-1]/(batch_idx+1)))\n",
    "        progbar.update(1)\n",
    "    train_loss[-1] = train_loss[-1]/len(train_loader)\n",
    "    \n",
    "def train_coach(epoch, net, coach, coach_optimizer):\n",
    "    progbar = tqdm_notebook(total=len(train_loader), desc='Coach')\n",
    "    coach.train()\n",
    "    net.eval()\n",
    "    coach_loss.append(0)\n",
    "    for batch_idx, (inputs_, masks, targets) in enumerate(train_loader):\n",
    "        coach_optimizer.zero_grad()\n",
    "        inputs_, targets = Variable(inputs_.cuda()), Variable(targets.cuda())\n",
    "        \n",
    "        masks, mu, logvar = coach.forward(inputs_, alpha = 1)\n",
    "            \n",
    "        outputs = net(inputs_*masks).detach()\n",
    "        mse_loss = (outputs - targets)**2\n",
    "        mse_loss = -1*F.threshold(-1*mse_loss, -2, -2)\n",
    "        loss_rec = torch.sum(mse_loss*(1-masks))/(3*torch.sum(1-masks))\n",
    "        \n",
    "        mu = mu.mean(dim = 2).mean(dim = 2)\n",
    "        logvar = logvar.mean(dim = 2).mean(dim = 2)\n",
    "\n",
    "        KLD = 0\n",
    "        try:\n",
    "            KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        except:\n",
    "            KLD = 0\n",
    "        \n",
    "        total_loss = 1-loss_rec + 1e-6*KLD\n",
    "        \n",
    "        \n",
    "        total_loss.backward()\n",
    "        coach_optimizer.step()\n",
    "        \n",
    "        coach_loss[-1] += total_loss.data\n",
    "        progbar.set_description('Coach (loss=%.4f)' % (coach_loss[-1]/(batch_idx+1)))\n",
    "        progbar.update(1)\n",
    "    coach_loss[-1] = coach_loss[-1]/len(train_loader)\n",
    "    \n",
    "        \n",
    "def val_context_inpainting(iter_, epoch, net, coach=None, use_coach_masks = False):\n",
    "    global best_loss\n",
    "    progbar = tqdm_notebook(total=len(val_loader), desc='Val')\n",
    "    net.eval()    \n",
    "    if coach is not None:\n",
    "        coach.eval()        \n",
    "    val_loss.append(0)\n",
    "    for batch_idx, (inputs_, masks, targets) in enumerate(val_loader):\n",
    "        inputs_, masks, targets = Variable(inputs_.cuda()), Variable(masks.cuda().float()), Variable(targets.cuda())    \n",
    "        \n",
    "        if coach is not None:\n",
    "            masks, _, _ = coach.forward(inputs_, alpha = 100, use_coach = use_coach_masks)\n",
    "        \n",
    "        outputs_1 = net(inputs_*masks)\n",
    "        mse_loss = (outputs_1 - targets)**2\n",
    "        mse_loss = -1*F.threshold(-1*mse_loss, -2, -2)\n",
    "        loss_rec = torch.sum(mse_loss*(1-masks))/torch.sum(1-masks)\n",
    "        \n",
    "        outputs_2 = net(inputs_*(1-masks))\n",
    "        mse_loss = (outputs_2 - targets)**2\n",
    "        mse_loss = -1*F.threshold(-1*mse_loss, -2, -2)\n",
    "        loss_con = torch.sum(mse_loss*masks)/torch.sum(masks)\n",
    "        \n",
    "        total_loss = rec_weight*loss_rec + (1-rec_weight)*loss_con\n",
    "        \n",
    "        val_loss[-1] += total_loss.data\n",
    "        progbar.set_description('Val (loss=%.4f)' % (val_loss[-1]/(batch_idx+1)))\n",
    "        progbar.update(1)\n",
    "        \n",
    "    val_loss[-1] = val_loss[-1]/len(val_loader)\n",
    "    if best_loss > val_loss[-1]:\n",
    "        best_loss = val_loss[-1]\n",
    "        print('Saving..')\n",
    "        state = {'context_inpainting_net': net, 'coach' : coach}\n",
    "        \n",
    "        torch.save(state, model_root + experiment + str(iter_) + '.ckpt.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_coach_masks = False\n",
    "epochs = []\n",
    "lrs = []\n",
    "\n",
    "if use_coach:\n",
    "    epochs = [100, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
    "    lrs = [[1e-1, 1e-2, 1e-3, 1e-4],\n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5],\n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5], \n",
    "       [1e-5, 1e-5, 1e-5, 1e-5]]\n",
    "else:\n",
    "    epochs = [100]\n",
    "    lrs = [[1e-1, 1e-2, 1e-3, 1e-4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progbar_1 = tqdm_notebook(total=len(epochs), desc='Iters')\n",
    "for iter_ in range(0, len(epochs)):\n",
    "    best_loss = 1e5    \n",
    "    \n",
    "    if use_coach and iter_ > 0:\n",
    "        use_coach_masks = True\n",
    "        progbar_2 = tqdm_notebook(total=epochs[iter_], desc='Epochs')\n",
    "        optimizer_coach = optim.Adam(net_coach.parameters(), lr=1e-5)\n",
    "        \n",
    "        for epoch in range(epochs[iter_]):\n",
    "            train_coach(epoch, net=net, coach=net_coach, coach_optimizer=optimizer_coach)\n",
    "            progbar_2.update(1)\n",
    "    \n",
    "    net_optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    progbar_2 = tqdm_notebook(total=epochs[iter_], desc='Epochs')\n",
    "    for epoch in range(epochs[iter_]):\n",
    "        if epoch%10 == 0:\n",
    "            if use_coach:\n",
    "                visualize_self_sup(cols = 4, net = net.eval(), coach = net_coach.eval(), use_coach_masks = use_coach_masks)\n",
    "            else:\n",
    "                visualize_self_sup(cols = 4, net = net.eval(), coach = None, use_coach_masks = use_coach_masks)\n",
    "                \n",
    "        if epoch == 90:\n",
    "            net_optimizer = optim.SGD(net.parameters(), lr=lrs[iter_][3], momentum=0.9, weight_decay=5e-4)\n",
    "        if epoch == 80:\n",
    "            net_optimizer = optim.SGD(net.parameters(), lr=lrs[iter_][2], momentum=0.9, weight_decay=5e-4)\n",
    "        if epoch == 40:\n",
    "            net_optimizer = optim.SGD(net.parameters(), lr=lrs[iter_][1], momentum=0.9, weight_decay=5e-4)\n",
    "        if epoch == 0:\n",
    "            net_optimizer = optim.SGD(net.parameters(), lr=lrs[iter_][0], momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "        train_context_inpainting(epoch, net = net, net_optimizer=net_optimizer, coach=net_coach, use_coach_masks=use_coach_masks)\n",
    "        val_context_inpainting(iter_, epoch, net = net, coach=net_coach, use_coach_masks=use_coach_masks)\n",
    "\n",
    "        progbar_2.update(1)\n",
    "                \n",
    "    progbar_1.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.printing import training_curves_loss\n",
    "training_curves_loss(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(net_coach)\n",
    "del(net)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FCNify_v2\n",
    "iter_ = len(epochs) - 1   ### iter_ = 0 is semantic inpainting model, iter_ > 0 is trained against coach masks\n",
    "net = torch.load(model_root + experiment + str(iter_) + '.ckpt.t7')['context_inpainting_net']\n",
    "net_segmentation = FCNify_v2(net, n_class = nClasses).cuda()\n",
    "optimizer_seg = None\n",
    "del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import soft_iou\n",
    "from metric import fast_hist, performMetrics\n",
    "from utils.dataloaders import segmentation_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_loss = []\n",
    "val_seg_loss = []\n",
    "train_seg_iou = []\n",
    "val_seg_iou = []\n",
    "ITER_SIZE = 2    ### accumulate gradients over ITER_SIZE iterations\n",
    "best_iou = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_loader = torch.utils.data.DataLoader(\n",
    "    segmentation_data_loader(img_root = train_img_root, gt_root = train_gt_root, image_list = train_image_list_path+supervised_split+'.txt',\n",
    "                             suffix=dataset, out=out, crop = True, crop_shape = [256, 256], mirror = True),\n",
    "                                           batch_size=32, num_workers=8, shuffle = True)\n",
    "\n",
    "val_seg_loader = torch.utils.data.DataLoader(\n",
    "    segmentation_data_loader(img_root = val_img_root, gt_root = val_gt_root, image_list = val_image_list,\n",
    "                             suffix=dataset, out=out, crop = False, mirror=False),\n",
    "                                           batch_size=8, num_workers=8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation(epoch, net_segmentation, seg_optimizer):\n",
    "    global train_seg_iou\n",
    "    progbar = tqdm_notebook(total=len(train_seg_loader), desc='Train')\n",
    "    net_segmentation.train()\n",
    "        \n",
    "    train_seg_loss.append(0)\n",
    "    seg_optimizer.zero_grad()\n",
    "    hist = np.zeros((nClasses, nClasses))\n",
    "    for batch_idx, (inputs_, targets) in enumerate(train_seg_loader):\n",
    "        inputs_, targets = Variable(inputs_.cuda()), Variable(targets.cuda())\n",
    "            \n",
    "        outputs = net_segmentation(inputs_)\n",
    "        \n",
    "        total_loss = (1 - soft_iou(outputs, targets, ignore=ignore_class))/ITER_SIZE   \n",
    "        total_loss.backward()\n",
    "        \n",
    "        if (batch_idx%ITER_SIZE == 0 and batch_idx!=0) or batch_idx==len(train_loader)-1:\n",
    "            seg_optimizer.step()\n",
    "            seg_optimizer.zero_grad()\n",
    "        \n",
    "        train_seg_loss[-1] += total_loss.data\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correctLabel = targets.view(-1, targets.size()[1], targets.size()[2])\n",
    "        hist += fast_hist(correctLabel.view(correctLabel.size(0),-1).cpu().numpy(),\n",
    "                        predicted.view(predicted.size(0),-1).cpu().numpy(),\n",
    "                        nClasses)\n",
    "        \n",
    "        miou, p_acc, fwacc = performMetrics(hist)\n",
    "        \n",
    "        progbar.set_description('Train (loss=%.4f, mIoU=%.4f)' % (train_seg_loss[-1]/(batch_idx+1), miou))\n",
    "        progbar.update(1)\n",
    "    train_seg_loss[-1] = train_seg_loss[-1]/len(train_seg_loader)\n",
    "    miou, p_acc, fwacc = performMetrics(hist)\n",
    "    train_seg_iou += [miou]\n",
    "    \n",
    "    \n",
    "def val_segmentation(epoch, net_segmentation):\n",
    "    global best_iou\n",
    "    global val_seg_iou\n",
    "    progbar = tqdm_notebook(total=len(val_seg_loader), desc='Val')\n",
    "    net_segmentation.eval()\n",
    "        \n",
    "    val_seg_loss.append(0)\n",
    "    hist = np.zeros((nClasses, nClasses))\n",
    "    for batch_idx, (inputs_, targets) in enumerate(val_seg_loader):\n",
    "        inputs_, targets = Variable(inputs_.cuda()), Variable(targets.cuda())\n",
    "            \n",
    "        outputs = net_segmentation(inputs_)\n",
    "        \n",
    "        total_loss = 1 - soft_iou(outputs, targets, ignore=ignore_class)\n",
    "        \n",
    "        val_seg_loss[-1] += total_loss.data\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correctLabel = targets.view(-1, targets.size()[1], targets.size()[2])\n",
    "        hist += fast_hist(correctLabel.view(correctLabel.size(0),-1).cpu().numpy(),\n",
    "                        predicted.view(predicted.size(0),-1).cpu().numpy(),\n",
    "                        nClasses)\n",
    "        \n",
    "        miou, p_acc, fwacc = performMetrics(hist)\n",
    "        \n",
    "        progbar.set_description('Val (loss=%.4f, mIoU=%.4f)' % (val_seg_loss[-1]/(batch_idx+1), miou))\n",
    "        progbar.update(1)\n",
    "    val_seg_loss[-1] = val_seg_loss[-1]/len(val_seg_loader)\n",
    "    val_miou, _, _ = performMetrics(hist)\n",
    "    val_seg_iou += [val_miou]\n",
    "\n",
    "    if best_iou < val_miou:\n",
    "        best_iou = val_miou\n",
    "        print('Saving..')\n",
    "        state = {'net_segmentation': net_segmentation}\n",
    "        \n",
    "        torch.save(state, model_root + experiment + 'segmentation' + '.ckpt.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progbar = tqdm_notebook(total=100, desc='Epochs')\n",
    "for epoch in range(0, 100):\n",
    "    if epoch == 90:\n",
    "        seg_optimizer = optim.SGD(net_segmentation.parameters(), lr=1e-6, momentum=0.9, weight_decay=5e-4)\n",
    "    elif epoch == 80:\n",
    "        seg_optimizer = optim.SGD(net_segmentation.parameters(), lr=1e-5, momentum=0.9, weight_decay=5e-4)\n",
    "    elif epoch == 60:\n",
    "        seg_optimizer = optim.SGD(net_segmentation.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "    elif epoch == 0:\n",
    "        seg_optimizer = optim.SGD(net_segmentation.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    train_segmentation(epoch, net_segmentation=net_segmentation, seg_optimizer=seg_optimizer)\n",
    "    val_segmentation(epoch, net_segmentation=net_segmentation)\n",
    "    progbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.printing import segmentation_training_curves_loss, apply_color_map\n",
    "segmentation_training_curves_loss(train_seg_loss, val_seg_loss, train_seg_iou, val_seg_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(net_segmentation)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_map = np.asarray([[128, 128, 128], [128, 128, 0], [0, 64, 0], [0, 128, 0], [128, 0, 0], [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(net_segmentation):\n",
    "    val_seg_loader = torch.utils.data.DataLoader(\n",
    "    segmentation_data_loader(img_root = val_img_root, gt_root = val_gt_root, image_list = val_image_list,\n",
    "                             suffix=dataset, out=out, crop = False, mirror=False),\n",
    "                                           batch_size=1, num_workers=8, shuffle = False)\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(9, 9))\n",
    "    for batch_idx, (inputs_, targets) in enumerate(val_seg_loader):\n",
    "        inputs_, targets = Variable(inputs_.cuda()), Variable(targets.cuda())\n",
    "            \n",
    "        outputs = net_segmentation(inputs_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        input_ = np.asarray(inputs_[0].cpu().numpy().transpose(1, 2, 0)+ mean_bgr[np.newaxis, np.newaxis, :], dtype=np.uint8)[:,:,::-1]\n",
    "        axs[batch_idx, 0].imshow(input_)\n",
    "        axs[batch_idx, 1].imshow(apply_color_map(targets[0].cpu().data, c_map))\n",
    "        axs[batch_idx, 2].imshow(apply_color_map(predicted[0].cpu().data, c_map))\n",
    "        if batch_idx == 3:\n",
    "            break\n",
    "     \n",
    "    axs[0,0].set_title('input', fontsize=18)\n",
    "    axs[0,1].set_title('GT', fontsize=18)\n",
    "    axs[0,2].set_title('Pred', fontsize=18)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def evaluate_segmentation(net_segmentation):\n",
    "    net_segmentation.eval()\n",
    "    hist = np.zeros((nClasses, nClasses))\n",
    "    val_seg_loader = torch.utils.data.DataLoader(\n",
    "    segmentation_data_loader(img_root = val_img_root, gt_root = val_gt_root, image_list = val_image_list,\n",
    "                             suffix=dataset, out=out, crop = False, mirror=False),\n",
    "                                           batch_size=1, num_workers=8, shuffle = False)\n",
    "    \n",
    "    progbar = tqdm_notebook(total=len(val_seg_loader), desc='Eval')\n",
    "        \n",
    "    hist = np.zeros((nClasses, nClasses))\n",
    "    for batch_idx, (inputs_, targets) in enumerate(val_seg_loader):\n",
    "        inputs_, targets = Variable(inputs_.cuda()), Variable(targets.cuda())\n",
    "            \n",
    "        outputs = net_segmentation(inputs_)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correctLabel = targets.view(-1, targets.size()[1], targets.size()[2])\n",
    "        hist += fast_hist(correctLabel.view(correctLabel.size(0),-1).cpu().numpy(),\n",
    "                        predicted.view(predicted.size(0),-1).cpu().numpy(),\n",
    "                        nClasses)\n",
    "        \n",
    "        miou, p_acc, fwacc = performMetrics(hist)\n",
    "        progbar.set_description('Eval (mIoU=%.4f)' % (miou))\n",
    "        progbar.update(1)\n",
    "        \n",
    "    miou, p_acc, fwacc = performMetrics(hist)\n",
    "    print('\\n mIoU: ', miou)\n",
    "    print('\\n Pixel accuracy: ', p_acc)\n",
    "    print('\\n Frequency Weighted Pixel accuracy: ', fwacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(model_root + experiment + 'segmentation' + '.ckpt.t7')['net_segmentation'].cuda().eval() ### load the best model\n",
    "evaluate_segmentation(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_segmentation(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
